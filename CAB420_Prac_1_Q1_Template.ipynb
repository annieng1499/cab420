{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annieng1499/cab420/blob/main/CAB420_Prac_1_Q1_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktc0lQcFZ-my"
      },
      "source": [
        "# CAB420, Practical 1 - Question 1 Template\n",
        "## Linear Regression\n",
        "\n",
        "Using the dataset from Problem 1, split the data into training, validation and testing as follows:\n",
        "* Training: All data from the years 2014-2016\n",
        "* Validation: All data from 2017\n",
        "* Training: All data from 2018\n",
        "\n",
        "Develop a regression model to predict one of the cycleway data series in your dataset. In developing this model you should:\n",
        "* Initially, use all weather data (temperature, rainfall and solar exposure) and all other data series for a particular counter type (i.e. if youâ€™re predicting cyclists inbound for a counter, use all other cyclist inbound counters)\n",
        "* Use p-values, qqplots, and performance on the validation set to remove terms and improve the model.\n",
        "\n",
        "When you have finished refining the model, evaluate it on test set, and compare the Root Mean Squared Error (RMSE) for the training, validation and test sets.\n",
        "\n",
        "In training the model, you will need to ensure that you have no samples (i.e. rows) with missing data. As such, you should remove samples with missing data from the dataset before training and evaluating the model. This may also mean that you have to remove some columns that contain large amounts of missing data.\n",
        "\n",
        "### Relevant Examples\n",
        "\n",
        "The first linear regression example, ``CAB420_Regression_Example_1_Linear_Regression.ipynb`` is a useful starting point here.\n",
        "\n",
        "### Suggested Packages\n",
        "\n",
        "The following packages are suggested, however there are many ways to approach things in python, if you'd rather use different pacakges that's cool too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCx87dgPZ-my"
      },
      "outputs": [],
      "source": [
        "# numpy handles pretty much anything that is a number/vector/matrix/array\n",
        "import numpy as np\n",
        "# pandas handles dataframes\n",
        "import pandas as pd\n",
        "# matplotlib emulates Matlabs plotting functionality\n",
        "import matplotlib.pyplot as plt\n",
        "# seaborn is another good plotting library. In particular, I like it for heatmaps (https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n",
        "import seaborn as sns;\n",
        "# stats models is a package that is going to perform the regression analysis\n",
        "from statsmodels import api as sm\n",
        "from scipy import stats\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# os allows us to manipulate variables on out local machine, such as paths and environment variables\n",
        "import os\n",
        "# self explainatory, dates and times\n",
        "from datetime import datetime, date\n",
        "# a helper package to help us iterate over objects\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryH5RNfVZ-mz"
      },
      "source": [
        "### Step 1: Load the data\n",
        "This may be the data you created in Q1, or the pre-baked merged data.\n",
        "\n",
        "Use pandas and the read_csv function to load the data. It is suggested you inspect the data after loading (print some of it, use the ``head()`` function, possibly plot some series) as a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0Xk7R2oZ-mz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W85QD73Z-mz"
      },
      "source": [
        "### Step 2: Filter the data\n",
        "\n",
        "As you inspect the data, you may see some series have fewer samples than others. Trying to find rows that have all data series may lead to having too little data for analysis. A suggested approach is:\n",
        "* Remove columns that have too many missin values. Use the ``dropna()`` function while setting ``axis=1`` and a threshold, such that only columns with threshold valid values or more are retained (a suggested value for threshold is about 1500, i.e. if any more than about 300 values are missing, the column should be removed).\n",
        "* Now, remove the other rows that have missing values. Here, you could use ``dropna()`` operating over the rows (i.e. ``axis=0``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-a31tFUZ-mz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehlKBNrJZ-mz"
      },
      "source": [
        "### Step 3: Split into Train, Validation and Test Splits\n",
        "\n",
        "You can split the data now. Be sure to check dataset size after splitting to make sure that you have datasets of roughly the size you expect.\n",
        "\n",
        "As part of this you should also pull out your X and Y data, i.e. your predictors and response.\n",
        "\n",
        "You could also visualise some of this data, and aspects such as:\n",
        "* Correlation between predictors and the response\n",
        "* Correlation between pairs of predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaxDPEdEZ-mz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBG6bu7jZ-mz"
      },
      "source": [
        "### Step 4: Create the Model\n",
        "\n",
        "Using the X and Y arrays you created above, fit a regression model.\n",
        "\n",
        "Explore the outputs you get from the model, including:\n",
        "* The resultant model, including coefficients, p-values, and $R^2$\n",
        "* A QQ-Plot, to see if assumptions around residuals hold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB8Qu7DcZ-mz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynW_bBVvZ-mz"
      },
      "source": [
        "### Step 5: Refine the Model, and Evaluate the results\n",
        "\n",
        "Based on model outputs and other data such as correlation, try to improve the model.\n",
        "\n",
        "Remove terms that look unhelpful. After a term is removed, evaluate the model on the validation and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSeHR-P3Z-mz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}